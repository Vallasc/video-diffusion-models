{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Video diffusion models"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:40:37.455618Z","iopub.status.busy":"2022-12-17T18:40:37.454734Z","iopub.status.idle":"2022-12-17T18:40:59.758931Z","shell.execute_reply":"2022-12-17T18:40:59.757239Z","shell.execute_reply.started":"2022-12-17T18:40:37.455508Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-12-17 18:40:38--  https://www.kaggleusercontent.com/kf/114071365/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..0XZ5rqxwM2Cimkc8L2potg.O4-iDoQ6h4vRCmX0yPixkteai54kwTj6H1XH64c4MjnL-X2_U6s29zWt2gtDQcWvHgJfxRTWTrFbdbGbP3cQf1B6GgsuTulNKwoq28dCSRh15U2QVXLKMpfgtdttpdeL6SPBHm5EE7HvKIKWTOpmYcyktcaIJD9fQGWjk2eHlP3VJjDZOuNlxJEBvRHHXzAC_72i6y23MsPvH90L4V1793Y388hoVnr3Jmxd7QxWcOzM3g3Q4EhC9LftbsSIIZ7VPtYoc2m9rlFOSpvXgG0PdQrDQb3Mshemx46gSBDNgbFOto7iSG1L-8UYstlhK5HBPgkSSyw7RnAay4-OdQktHKcXB1Kb798OMKtzthgJLOKzxuY0TUJS4wdwLDDqrkOzGCAjINRVsaDyOKkj6MAVsGnbGqNzbd3vQ3KmRGcRSz7ZMbWH3evRaMedxVzorx_EWtAbVzpgQoXb3MK27d5UNKs5EFUMDv7pvPGJV0cAK_dYqhcQetpyMC0RTtcHkBn74sL3YqedRrMm-F5wNzM_dVaV25jQOBA2QTmD5U3357uGY3tc7rT3HBvXdUXrk6fik7DI-PXg3kuggNalKZ735nq2A1dN-D3OhgCSVxjL4a_o42IRCI03c8AgaXDUUM6GQGE_SF6dBcS0458X758uWA.hnnVI8QuuCcymLi78ZVVJw/checkpoint-unet_2-epoch_2-step_125782-1671301290.pt\n","Resolving www.kaggleusercontent.com (www.kaggleusercontent.com)... 35.190.26.106\n","Connecting to www.kaggleusercontent.com (www.kaggleusercontent.com)|35.190.26.106|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3089317316 (2.9G) [application/x-zip]\n","Saving to: ‘checkpoint-unet_2-epoch_2-step_125782-1671301290.pt’\n","\n","checkpoint-unet_2-e 100%[===================>]   2.88G   152MB/s    in 21s     \n","\n","2022-12-17 18:40:59 (143 MB/s) - ‘checkpoint-unet_2-epoch_2-step_125782-1671301290.pt’ saved [3089317316/3089317316]\n","\n"]}],"source":["# Here wget old checkpoint\n","# !wget ..."]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:40:59.764741Z","iopub.status.busy":"2022-12-17T18:40:59.764222Z","iopub.status.idle":"2022-12-17T18:40:59.778392Z","shell.execute_reply":"2022-12-17T18:40:59.775863Z","shell.execute_reply.started":"2022-12-17T18:40:59.764685Z"},"trusted":true},"outputs":[],"source":["# Params\n","image_size = 128\n","frames = 12\n","download_batch_size = 128\n","download_workers = 20\n","\n","dataset_size = 125782\n","\n","# If there is a checkpoint these changes automatically at runtime\n","epoch = 1\n","train_unet = 1\n","current_step = 0"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:40:59.781069Z","iopub.status.busy":"2022-12-17T18:40:59.780711Z","iopub.status.idle":"2022-12-17T18:40:59.794453Z","shell.execute_reply":"2022-12-17T18:40:59.792917Z","shell.execute_reply.started":"2022-12-17T18:40:59.781032Z"},"trusted":true},"outputs":[],"source":["import time\n","import os\n","start_time = time.time()\n","\n","# Clean tmp files\n","for file in os.listdir(\"./\"):\n","    if file.endswith('.gif'):\n","        os.remove(file)"]},{"cell_type":"markdown","metadata":{},"source":["## Install dependencies"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:40:59.801615Z","iopub.status.busy":"2022-12-17T18:40:59.798854Z","iopub.status.idle":"2022-12-17T18:41:13.321450Z","shell.execute_reply":"2022-12-17T18:41:13.320297Z","shell.execute_reply.started":"2022-12-17T18:40:59.801572Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting imagen_pytorch==1.16.5\n","  Downloading imagen_pytorch-1.16.5-py3-none-any.whl (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (2.1.0)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (0.1.97)\n","Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (0.12.0)\n","Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (8.0.4)\n","Requirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (1.7.7)\n","Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (4.20.1)\n","Requirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (1.8.2)\n","Requirement already satisfied: accelerate in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (0.12.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (21.3)\n","Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (9.1.1)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (4.64.0)\n","Collecting einops-exts\n","  Downloading einops_exts-0.0.3-py3-none-any.whl (3.8 kB)\n","Collecting einops>=0.6\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-warmup\n","  Downloading pytorch_warmup-0.1.1-py3-none-any.whl (6.6 kB)\n","Collecting ema-pytorch>=0.0.3\n","  Downloading ema_pytorch-0.1.2-py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (2022.8.2)\n","Collecting beartype\n","  Downloading beartype-0.11.0-py3-none-any.whl (702 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m702.5/702.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (1.21.6)\n","Requirement already satisfied: torch>=1.6 in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (1.11.0)\n","Requirement already satisfied: kornia in /opt/conda/lib/python3.7/site-packages (from imagen_pytorch==1.16.5) (0.5.8)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.6->imagen_pytorch==1.16.5) (4.1.1)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from accelerate->imagen_pytorch==1.16.5) (6.0)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from accelerate->imagen_pytorch==1.16.5) (5.9.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->imagen_pytorch==1.16.5) (3.0.9)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->imagen_pytorch==1.16.5) (4.13.0)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (2.28.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (0.10.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (3.8.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (0.70.13)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (3.0.0)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (0.18.0)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (5.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (0.3.5.1)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets->imagen_pytorch==1.16.5) (1.3.5)\n","Requirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.16.5) (0.3.2)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.16.5) (0.10.0)\n","Requirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.7/site-packages (from pytorch-lightning->imagen_pytorch==1.16.5) (2.10.1)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.16.5) (3.7.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.16.5) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers->imagen_pytorch==1.16.5) (2021.11.10)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (1.7.2)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (21.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (1.3.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (2.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (6.0.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets->imagen_pytorch==1.16.5) (1.2.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.16.5) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.16.5) (1.26.12)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets->imagen_pytorch==1.16.5) (3.3)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (2.2.2)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.6.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.4.6)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (3.19.4)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (1.43.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (3.3.7)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (59.8.0)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.37.1)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->imagen_pytorch==1.16.5) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->imagen_pytorch==1.16.5) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets->imagen_pytorch==1.16.5) (2022.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.2.7)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (2.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning->imagen_pytorch==1.16.5) (3.2.0)\n","Installing collected packages: einops, beartype, pytorch-warmup, ema-pytorch, einops-exts, imagen_pytorch\n","Successfully installed beartype-0.11.0 einops-0.6.0 einops-exts-0.0.3 ema-pytorch-0.1.2 imagen_pytorch-1.16.5 pytorch-warmup-0.1.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install imagen_pytorch==1.16.5 --no-cache-dir"]},{"cell_type":"markdown","metadata":{},"source":["## Utility functions to resize and crop GIFs"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:41:13.324067Z","iopub.status.busy":"2022-12-17T18:41:13.323646Z","iopub.status.idle":"2022-12-17T18:41:15.238746Z","shell.execute_reply":"2022-12-17T18:41:15.237746Z","shell.execute_reply.started":"2022-12-17T18:41:13.324027Z"},"tags":[],"trusted":true},"outputs":[],"source":["# GIF pre-processing\n","\n","import numpy as np\n","from torchvision import transforms as T\n","from math import floor, fabs\n","from PIL import Image, ImageSequence\n","\n","\n","CHANNELS_TO_MODE = {\n","    1 : 'L',\n","    3 : 'RGB',\n","    4 : 'RGBA'\n","}\n","\n","def center_crop(img, new_width, new_height): \n","    width = img.size[0]\n","    height = img.size[1]\n","    left = int(np.ceil((width - new_width) / 2))\n","    right = width - int(np.floor((width - new_width) / 2))\n","    top = int(np.ceil((height - new_height) / 2))\n","    bottom = height - int(np.floor((height - new_height) / 2))\n","    return img.crop((left, top, right, bottom))\n","\n","def resize_crop_img(img, width, height):\n","    # width < height\n","    if( img.size[0] < img.size[1]):\n","        wpercent = (width/float(img.size[0]))\n","        hsize = int((float(img.size[1])*float(wpercent)))\n","        img = img.resize((width, hsize), Image.Resampling.LANCZOS)\n","    else: # width >= height\n","        hpercent = (height/float(img.size[1]))\n","        wsize = int((float(img.size[0])*float(hpercent)))\n","        img = img.resize((wsize, height), Image.Resampling.LANCZOS)\n","    img = center_crop(img, width, height)\n","    # print(img.size[0])\n","    # print(img.size[1])\n","    return img\n","\n","def transform_gif(img, new_width, new_height, frames, channels = 3):\n","    assert channels in CHANNELS_TO_MODE, f'channels {channels} invalid'\n","    mode = CHANNELS_TO_MODE[channels]\n","    gif_frames = img.n_frames\n","    for i in range(0, frames):\n","        img.seek(i % gif_frames)\n","        img_out = resize_crop_img(img, new_width, new_height)\n","        yield img_out.convert(mode)\n","        \n","# tensor of shape (channels, frames, height, width) -> gif\n","def video_tensor_to_gif(tensor, path, fps = 10, loop = 0, optimize = True):\n","    print(\"Converting video tensors to GIF\")\n","    images = map(T.ToPILImage(), tensor.unbind(dim = 1))\n","    first_img, *rest_imgs = images\n","    print(1000/fps)\n","    first_img.save(path, save_all = True, append_images = rest_imgs, duration = int(1000/fps), loop = loop, optimize = optimize)\n","    print(\"Gif saved\")\n","    return images\n","\n","# gif -> (channels, frame, height, width) tensor\n","def gif_to_tensor(path, width = 256, height = 256, frames = 32, channels = 3, transform = T.ToTensor()):\n","    print(\"Converting GIF to video tensors\")\n","    img = Image.open(path)\n","    imgs = transform_gif(img, new_width = width, new_height = height, frames = frames, channels = channels)\n","    tensors = tuple(map(transform, imgs))\n","    return torch.stack(tensors, dim = 1)"]},{"cell_type":"markdown","metadata":{},"source":["## Utility functions to download dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:41:15.240668Z","iopub.status.busy":"2022-12-17T18:41:15.240025Z","iopub.status.idle":"2022-12-17T18:41:15.731166Z","shell.execute_reply":"2022-12-17T18:41:15.730185Z","shell.execute_reply.started":"2022-12-17T18:41:15.240613Z"},"tags":[],"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import shutil\n","import urllib\n","\n","from concurrent.futures import ThreadPoolExecutor, wait\n","import time\n","import threading\n","\n","train_url = \"https://raw.githubusercontent.com/raingo/TGIF-Release/master/data/tgif-v1.0.tsv\"\n","train_data = \"./train_data.tvs\"\n","\n","current_step = 0\n","texts = []\n","list_videos = []\n","\n","\n","def download_url(url, root, filename=None):\n","    \"\"\"Download a file from a url and place it in root.\n","    Args:\n","        url (str): URL to download file from\n","        root (str): Directory to place downloaded file in\n","        filename (str, optional): Name to save the file under. If None, use the basename of the URL\n","    \"\"\"\n","    root = os.path.expanduser(root)\n","    if not filename:\n","        filename = os.path.basename(url)\n","    fpath = os.path.join(root, filename)\n","\n","    os.makedirs(root, exist_ok=True)\n","\n","    try:\n","        #print('Downloading ' + url + ' to ' + fpath)\n","        urllib.request.urlretrieve(url, fpath)\n","    except (urllib.error.URLError, IOError) as e:\n","        if url[:5] == 'https':\n","            url = url.replace('https:', 'http:')\n","            print('Failed download. Trying https -> http instead.'\n","                    ' Downloading ' + url + ' to ' + fpath)\n","            urllib.request.urlretrieve(url, fpath)\n","\n","def get_videos(index_start, index_end):\n","    global texts\n","    global list_videos\n","    \n","    texts = []\n","    list_videos = []\n","\n","    with open(\"train_data.tvs\") as fp:\n","        for i, line in enumerate(fp):\n","            if i >= index_start and i< index_end :\n","                try:\n","                    file_img, file_text = line.split(\"\\t\")\n","                    print(f\"Downloading image {i}\")\n","                    download_url(file_img, \"./\", \"download.gif\")\n","                    tensor = gif_to_tensor('download.gif', width = image_size, height = image_size, frames = frames)\n","                    list_videos.append(tensor)\n","                    file_text = file_text[:-1] # Remove \\n\n","                    texts.append(file_text)\n","                    os.remove('download.gif')\n","                except Exception as ex:\n","                    print(ex)\n","                    pass\n","            elif i > index_end:\n","                break\n","\n","lock = threading.Lock()\n","executor = ThreadPoolExecutor(max_workers=download_workers)\n","\n","def download_process_parallel(index, file_img, file_text):\n","    try:\n","        print(f\"Downloading image {index}\")\n","        download_url(file_img, \"./\", f\"{index}.gif\")\n","        tensor = gif_to_tensor(f\"{index}.gif\", width = image_size, height = image_size, frames = frames)\n","        file_text = file_text[:-1] # Remove \\n\n","        with lock:\n","            list_videos.append(tensor)\n","            texts.append(file_text)\n","        os.remove(f\"{index}.gif\")\n","    except Exception as ex:\n","        print(ex)\n","        pass\n","    \n","def get_videos_parallel(index_start, index_end):\n","    global texts\n","    global list_videos\n","    \n","    texts = []\n","    list_videos = []\n","\n","    with open(\"train_data.tvs\") as fp:\n","        futures = []\n","        for i, line in enumerate(fp):\n","            if i >= index_start and i< index_end :\n","                file_img, file_text = line.split(\"\\t\")\n","                future = executor.submit(download_process_parallel, i, file_img, file_text)\n","                futures.append(future)\n","            elif i > index_end:\n","                break\n","        wait(futures)\n","                \n","def get_next_videos():\n","    global current_step\n","    get_videos_parallel(current_step, current_step + download_batch_size)\n","    current_step += len(texts)\n","\n","# Download train data file\n","if not os.path.exists(train_data):\n","    download_url(train_url, \"./\", train_data)"]},{"cell_type":"markdown","metadata":{},"source":["## Utility functions to save and load checkpoints"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:41:15.734265Z","iopub.status.busy":"2022-12-17T18:41:15.733547Z","iopub.status.idle":"2022-12-17T18:41:18.207340Z","shell.execute_reply":"2022-12-17T18:41:18.206341Z","shell.execute_reply.started":"2022-12-17T18:41:15.734223Z"},"tags":[],"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f45fe2586cb475cb7dc1f6e8495d53a","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/605 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import shutil\n","import torch\n","import time\n","import gc\n","import os\n","from imagen_pytorch import ImagenTrainer\n","from imagen_pytorch.data import Dataset\n","\n","checkpoints_path = \"./\"\n","checkpoint_path = \"\"\n","\n","def save_checkpoint(trainer: ImagenTrainer):\n","    global checkpoint_path\n","    print(\"Saving checkpoint\")\n","    current_time = int(time.time())\n","    if os.path.exists(checkpoint_path):\n","        os.remove(checkpoint_path)\n","    checkpoint_path = os.path.join(checkpoints_path, f\"checkpoint-unet_{train_unet}-epoch_{epoch}-step_{current_step}-{current_time}.pt\")\n","    trainer.save(checkpoint_path)\n","\n","def update_config(checkpoint):\n","    global epoch\n","    global train_unet\n","    global current_step\n","    splitted = (checkpoint.replace(\".pt\", \"\").split(\"checkpoint-\")[1]).split(\"-\")\n","    train_unet = int(splitted[0].replace(\"unet_\", \"\"))\n","    epoch = int(splitted[1].replace(\"epoch_\", \"\"))\n","    current_step = int(splitted[2].replace(\"step_\", \"\"))\n","    print(\"Loaded configuration\")\n","    print(f\"Step: {current_step}\")\n","    print(f\"Epoch: {epoch}\")\n","    print(f\"Unet: {train_unet}\")\n","    if current_step >= dataset_size:\n","        if train_unet == 2:\n","            print(\"End of training Unet 2 -> New epoch\")\n","            train_unet = 1\n","            epoch += 1\n","        else:\n","            print(\"End of training Unet 1\")\n","            train_unet = 2\n","        current_step = 0\n","        print(\"New configuration\")\n","        print(f\"Step: {current_step}\")\n","        print(f\"Epoch: {epoch}\")\n","        print(f\"Unet: {train_unet}\")\n","        \n","def load_checkpoint(trainer: ImagenTrainer):\n","    global checkpoint_path\n","    print(\"Loading checkpoint\")\n","    timestamp = -1\n","    for file in os.listdir(checkpoints_path):\n","        if file.endswith('.pt'):\n","            new_timestamp = int((file.split(\"-\")[4]).replace(\".pt\", \"\"))\n","            if new_timestamp > timestamp:\n","                checkpoint_path = os.path.join(checkpoints_path, file)\n","                timestamp = new_timestamp        \n","    if not os.path.exists(checkpoint_path):\n","        print(\"No checkpoint found -> starting from scratch\")\n","        return None\n","    trainer.load(checkpoint_path)\n","    update_config(checkpoint_path)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:41:18.209883Z","iopub.status.busy":"2022-12-17T18:41:18.209167Z","iopub.status.idle":"2022-12-17T18:41:27.036084Z","shell.execute_reply":"2022-12-17T18:41:27.034945Z","shell.execute_reply.started":"2022-12-17T18:41:18.209836Z"},"tags":[],"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["The base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n"]}],"source":["from imagen_pytorch import Unet3D, ElucidatedImagen, ImagenTrainer\n","\n","unet1 = Unet3D(\n","    dim = 64,\n","    cond_dim = 128,\n","    dim_mults = (1, 2, 4, 8),\n","    num_resnet_blocks = 3,\n","    layer_attns = (False, True, True, True),\n","    layer_cross_attns = (False, True, True, True)\n",")\n","\n","unet2 = Unet3D(\n","    dim = 64,\n","    cond_dim = 128,\n","    dim_mults = (1, 2, 4, 8),\n","    num_resnet_blocks = (2, 4, 8, 8),\n","    layer_attns = (False, False, False, True),\n","    layer_cross_attns = (False, False, False, True)\n",")\n","\n","imagen = ElucidatedImagen(\n","    unets = (unet1, unet2),\n","    image_sizes = (16, 64),\n","    random_crop_sizes = (None, 16),\n","    num_sample_steps = 64,\n","    cond_drop_prob = 0.1,                       # gives the probability of dropout for classifier-free guidance.\n","    sigma_min = 0.002,                          # min noise level\n","    sigma_max = (80, 160),                      # max noise level, double the max noise level for upsampler\n","    sigma_data = 0.5,                           # standard deviation of data distribution\n","    rho = 7,                                    # controls the sampling schedule\n","    P_mean = -1.2,                              # mean of log-normal distribution from which noise is drawn for training\n","    P_std = 1.2,                                # standard deviation of log-normal distribution from which noise is drawn for training\n","    S_churn = 80,                               # parameters for stochastic sampling - depends on dataset, Table 5 in apper\n","    S_tmin = 0.05,\n","    S_tmax = 50,\n","    S_noise = 1.003,\n",").cuda()\n","\n","trainer = ImagenTrainer(imagen)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Train Unet"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:41:27.043758Z","iopub.status.busy":"2022-12-17T18:41:27.041281Z","iopub.status.idle":"2022-12-17T18:41:27.050993Z","shell.execute_reply":"2022-12-17T18:41:27.049847Z","shell.execute_reply.started":"2022-12-17T18:41:27.043717Z"},"scrolled":true,"tags":[],"trusted":true},"outputs":[],"source":["# Train \n","load_checkpoint(trainer)\n","# counter = 0\n","while True:\n","    # if execution time is more than 11h40m stops\n","    if time.time() - start_time > 42000:\n","        break\n","    get_next_videos()\n","    # if there are no more videos stops\n","    if len(texts) == 0:\n","        break\n","    print(\"Generating tensor from videos\")\n","    videos = torch.stack(list_videos, dim = 0).cuda()\n","    print(f\"Training Unet {train_unet}\")\n","    trainer(videos, texts = texts, unet_number = train_unet, max_batch_size = 32)\n","    trainer.update(unet_number = train_unet)\n","    del videos\n","#     if counter % 100 == 0:\n","#         break\n","save_checkpoint(trainer)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:44:16.214594Z","iopub.status.busy":"2022-12-17T18:44:16.214205Z","iopub.status.idle":"2022-12-17T18:45:36.351690Z","shell.execute_reply":"2022-12-17T18:45:36.350567Z","shell.execute_reply.started":"2022-12-17T18:44:16.214561Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["checkpoint loaded from ./checkpoint-unet_2-epoch_2-step_125782-1671301290.pt\n","['unet_2', 'epoch_2', 'step_125782', '1671301290']\n","Loaded configuration\n","Step: 125782\n","Epoch: 2\n","Unet: 2\n","End of training Unet 2 -> New epoch\n","New configuration\n","Step: 0\n","Epoch: 3\n","Unet: 1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c575578d61a54b3b9b4c66017dca01b9","version_major":2,"version_minor":0},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad7584aa6ff541ef845d8ba7881dc043","version_major":2,"version_minor":0},"text/plain":["sampling time step:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18a0f8e0c16f4ff08825dfdb9a4c3fed","version_major":2,"version_minor":0},"text/plain":["sampling time step:   0%|          | 0/64 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["torch.Size([1, 3, 24, 64, 64])\n","Converting video tensors to GIF\n","200.0\n","Gif saved\n"]},{"data":{"text/plain":["<map at 0x7fe35fa89f90>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# texts_sample = ['person']\n","# load_checkpoint(trainer)\n","# videos_out = trainer.sample(texts = texts_sample, video_frames = 24)\n","# print(videos_out.shape)\n","# video_tensor_to_gif(videos_out[0], f'out.gif', fps = 5)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:43:05.074594Z","iopub.status.busy":"2022-12-17T18:43:05.074211Z","iopub.status.idle":"2022-12-17T18:43:05.079536Z","shell.execute_reply":"2022-12-17T18:43:05.078404Z","shell.execute_reply.started":"2022-12-17T18:43:05.074556Z"},"trusted":true},"outputs":[],"source":["# !pip install GPUtil\n","\n","# from GPUtil import showUtilization as gpu_usage\n","# gpu_usage()    "]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-17T18:43:05.081558Z","iopub.status.busy":"2022-12-17T18:43:05.081205Z","iopub.status.idle":"2022-12-17T18:43:05.089553Z","shell.execute_reply":"2022-12-17T18:43:05.088633Z","shell.execute_reply.started":"2022-12-17T18:43:05.081524Z"},"trusted":true},"outputs":[],"source":["#end"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"82870c2f554ac463822076b994cf0f6148cb66eef85ce34abb8e9fcae1bddbbc"}}},"nbformat":4,"nbformat_minor":4}
